{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58d51d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pickle import load\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "#from pickle import dumptaged\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences \n",
    "from keras.models import load_model\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding,LSTM,RepeatVector,TimeDistributed,Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from numpy.random import rand\n",
    "from numpy.random import shuffle\n",
    "import unicodedata\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15701ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add path to the json file\n",
    "data_path = 'C:\\\\Users\\\\welcome\\\\Desktop\\\\PYTHON\\\\andrew ng\\\\yugasa\\\\hinglish to English\\\\git demo\\\\English-to-Hinglish-Neural-Machine-Translation\\hinglish_upload_v1.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f72b4d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []  # Empty list to store dictionaries\n",
    "\n",
    "with open(data_path, 'r') as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            obj = json.loads(line)\n",
    "            dictionary = {\n",
    "              'en': obj['translation']['en'],\n",
    "              'hi_en': obj['translation']['hi_ng'],\n",
    "                # Add more key-value pairs as needed\n",
    "            }\n",
    "            data.append(dictionary)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON: {e}\")\n",
    "\n",
    "# Now 'data' contains an array of dictionaries from json file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3717efd0",
   "metadata": {},
   "outputs": [],
   "source": [
    " del dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89b33e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I haven't seen a lot of the old classics for a long time. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "189102"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data[66]['en'])\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "484f68c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To seprate the two languages into different lists\n",
    "\n",
    "en = []\n",
    "hi_en = []\n",
    "\n",
    "for dictionary in data:\n",
    "    # Extract values for key1, key2, and key3\n",
    "        value1 = dictionary['en']\n",
    "        value2 = dictionary['hi_en']\n",
    "\n",
    "        # Append values to respective arrays\n",
    "        en.append(value1)\n",
    "        hi_en.append(value2)\n",
    "\n",
    "      \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c50475bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(en[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "849c4768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English-- What's the name of the movie\n",
      "Hinglish-- film ka kya naam hai\n",
      "________________________________________________\n",
      "English-- Hi, the rotten tomatoes score is great but the meta critic score seems a little low a movie of this quality. \n",
      "Hinglish-- namaste, sada hua tomatoes score mahaan hai, lekin meta critic score is gunavatta kee philm se thoda kam lagata hai.\n",
      "________________________________________________\n",
      "English-- Do you think you will like the movie\n",
      "Hinglish-- kya aapako lagata hai ki aapako film pasand aaegee\n",
      "________________________________________________\n",
      "189102\n",
      "189102\n"
     ]
    }
   ],
   "source": [
    "# to check if spliting the data was done correctly\n",
    "for i in range(3):\n",
    "    print('English--',en[i])\n",
    "    print('Hinglish--',hi_en[i])\n",
    "    print('________________________________________________')\n",
    "    \n",
    "print(len(en))\n",
    "print(len(hi_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0df9ecb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#zipping the two list into one numpy array\n",
    "import numpy as np\n",
    "pairs= np.array(list(zip(hi_en,en)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b507b0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['film ka kya naam hai' \"What's the name of the movie\"]\n",
      " ['namaste, sada hua tomatoes score mahaan hai, lekin meta critic score is gunavatta kee philm se thoda kam lagata hai.'\n",
      "  'Hi, the rotten tomatoes score is great but the meta critic score seems a little low a movie of this quality. ']\n",
      " ['kya aapako lagata hai ki aapako film pasand aaegee'\n",
      "  'Do you think you will like the movie']\n",
      " ...\n",
      " ['Me: Aaj kuch toofani karte hai.\\n\\nMom: Pani ki bottles bhar ke fridge mein rakh de.'\n",
      "  \"Me: Let's do some stormy today.\\n\\nMom: Fill the water bottles and keep them in the fridge.\"]\n",
      " ['Pyar mangi to Jaan dengi,milk mango to kher dengi,agaar Kashmir mangi to chir dengi.'\n",
      "  \"Ask for love we'll give life, ask for milk we'll give porridge, ask for Kashmir we'll tear you\"]\n",
      " [\"Ye sab aunty'on ke saath?\" 'All this with aunties?']]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(threshold=10000)\n",
    "print(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b730948",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#data cleaning\n",
    "# clean a list of lines\n",
    "def clean_pairs(lines):\n",
    "\tcleaned = list()\n",
    "\t# prepare regex for char filtering\n",
    "\tre_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "\t# prepare translation table for removing punctuation\n",
    "\ttable = str.maketrans('', '', string.punctuation)\n",
    "\tfor pair in lines:\n",
    "\t\tclean_pair = list()\n",
    "\t\tfor line in pair:\n",
    "\t\t\t# normalize unicode characters\n",
    "\t\t\tline = unicodedata.normalize('NFD', line).encode('ascii', 'ignore')\n",
    "\t\t\tline = line.decode('UTF-8')\n",
    "\t\t\t# tokenize on white space\n",
    "\t\t\tline = line.split()\n",
    "\t\t\t# convert to lowercase\n",
    "\t\t\tline = [word.lower() for word in line]\n",
    "\t\t\t# remove punctuation from each token\n",
    "\t\t\tline = [word.translate(table) for word in line]\n",
    "\t\t\t# remove non-printable chars form each token\n",
    "\t\t\tline = [re_print.sub('', w) for w in line]\n",
    "\t\t\t# remove tokens with numbers in them\n",
    "\t\t\tline = [word for word in line if word.isalpha()]\n",
    "\t\t\t# store as string\n",
    "\t\t\tclean_pair.append(' '.join(line))\n",
    "\t\tcleaned.append(clean_pair)\n",
    "\treturn array(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf7489f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clean_pairs = clean_pairs(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae9443f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed rows: []\n"
     ]
    }
   ],
   "source": [
    "#  Check if the length of the source sentence (column at index 0) is greater than 0\n",
    "non_empty_rows = clean_pairs[:, 0] != ''\n",
    "\n",
    "# Remove the rows where the length is 0\n",
    "clean_pairs = clean_pairs[non_empty_rows, :]\n",
    "\n",
    "# Print the removed rows\n",
    "removed_rows = clean_pairs[~non_empty_rows, 0]\n",
    "print(\"Removed rows:\", removed_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da3eecb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, line in enumerate(clean_pairs):\n",
    "    # Check if the length of the source sentence (column at index 0) is greater than 7\n",
    "    \n",
    "    if len(clean_pairs[i, 0]) == 0:\n",
    "        # Index of the row you want to remove\n",
    "        print('--',clean_pairs[i, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c388d334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['film ka kya naam hai' 'whats the name of the movie']\n",
      " ['namaste sada hua tomatoes score mahaan hai lekin meta critic score is gunavatta kee philm se thoda kam lagata hai'\n",
      "  'hi the rotten tomatoes score is great but the meta critic score seems a little low a movie of this quality']\n",
      " ['kya aapako lagata hai ki aapako film pasand aaegee'\n",
      "  'do you think you will like the movie']\n",
      " ...\n",
      " ['me aaj kuch toofani karte hai mom pani ki bottles bhar ke fridge mein rakh de'\n",
      "  'me lets do some stormy today mom fill the water bottles and keep them in the fridge']\n",
      " ['pyar mangi to jaan dengimilk mango to kher dengiagaar kashmir mangi to chir dengi'\n",
      "  'ask for love well give life ask for milk well give porridge ask for kashmir well tear you']\n",
      " ['ye sab auntyon ke saath' 'all this with aunties']]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(threshold=100000)\n",
    "print(clean_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c31923b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "273\n",
      "247\n"
     ]
    }
   ],
   "source": [
    "def max_length(lines):\n",
    "\treturn max(len(line.split()) for line in lines)\n",
    "\n",
    "print(max_length(clean_pairs[:, 0]))\n",
    "print(max_length(clean_pairs[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bfb080a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['film ka kya naam hai',\n",
       "       'namaste sada hua tomatoes score mahaan hai lekin meta critic score is gunavatta kee philm se thoda kam lagata hai',\n",
       "       'kya aapako lagata hai ki aapako film pasand aaegee', ...,\n",
       "       'me aaj kuch toofani karte hai mom pani ki bottles bhar ke fridge mein rakh de',\n",
       "       'pyar mangi to jaan dengimilk mango to kher dengiagaar kashmir mangi to chir dengi',\n",
       "       'ye sab auntyon ke saath'], dtype='<U1463')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_pairs[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b3697b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#settig the sentence lenth \n",
    "# Create an empty list to store the indices of rows to delete\n",
    "rows_to_delete = []\n",
    "\n",
    "for i, line in enumerate(clean_pairs):\n",
    "    # Check if the length of the source sentence (column at index 0) is greater than 7\n",
    "    if len(clean_pairs[i, 0]) > 20:\n",
    "        # If the condition is met, add the index 'i' to the list of rows to delete\n",
    "        rows_to_delete.append(i)\n",
    "\n",
    "# Delete the rows specified by 'rows_to_delete'\n",
    "new_data = np.delete(clean_pairs, rows_to_delete, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5d9b7431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12537\n"
     ]
    }
   ],
   "source": [
    "print(len(new_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26f9a3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12537\n"
     ]
    }
   ],
   "source": [
    "clean_pairs = new_data\n",
    "print(len(clean_pairs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6cce20e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: english-hi.pkl\n"
     ]
    }
   ],
   "source": [
    "def save_clean_data(sentences, filename):\n",
    " dump(sentences, open(filename, 'wb'))\n",
    " print('Saved: %s' % filename)\n",
    " t\n",
    "\n",
    "save_clean_data(clean_pairs, 'english-hi.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eff909cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: english-hi-both.pkl\n",
      "Saved: english-hi-train.pkl\n",
      "Saved: english-hi-test.pkl\n"
     ]
    }
   ],
   "source": [
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    " return load(open(filename, 'rb'))\n",
    " \n",
    "# save a list of clean sentences to file\n",
    "def save_clean_data(sentences, filename):\n",
    " dump(sentences, open(filename, 'wb'))\n",
    " print('Saved: %s' % filename)\n",
    " \n",
    "# load dataset\n",
    "raw_dataset = load_clean_sentences('english-hi.pkl')\n",
    " \n",
    "# reduce dataset size\n",
    "n_sentences = 8000\n",
    "dataset = raw_dataset[:n_sentences, :]\n",
    "# random shuffle\n",
    "shuffle(dataset)\n",
    "# split into train/test\n",
    "train, test = dataset[:7200], dataset[7200:]\n",
    "# save\n",
    "save_clean_data(dataset, 'english-hi-both.pkl')\n",
    "save_clean_data(train, 'english-hi-train.pkl')\n",
    "save_clean_data(test, 'english-hi-test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f48679",
   "metadata": {},
   "source": [
    "# Train Neural Translation Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "21d0a02f",
   "metadata": {},
   "outputs": [],
   "source": [
    " # load datasets\n",
    "dataset = load_clean_sentences('english-hi-both.pkl')\n",
    "train = load_clean_sentences('english-hi-train.pkl')\n",
    "test = load_clean_sentences('english-hi-test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8164b0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.str_'>\n",
      "<class 'numpy.str_'>\n",
      "<class 'numpy.str_'>\n",
      "98\n",
      "8000\n",
      "7200\n",
      "800\n"
     ]
    }
   ],
   "source": [
    "print(type(dataset[1,1]))\n",
    "print(type(test[1,1]))\n",
    "print(type(train[1,1]))\n",
    "import sys\n",
    "print(sys.getsizeof(dataset[1,1]))\n",
    "print(len(dataset))\n",
    "print(len(train))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ebdd15a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "\ttokenizer = Tokenizer()\n",
    "\ttokenizer.fit_on_texts(lines)\n",
    "\treturn tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c7a603bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max sentence length\n",
    "def max_length(lines):\n",
    "\treturn max(len(line.split()) for line in lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0aef0b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hinglish Vocabulary Size: 2100\n",
      "Hinglish Max Length: 6\n",
      "English Vocabulary Size: 1807\n",
      "English Max Length: 13\n"
     ]
    }
   ],
   "source": [
    "# prepare english tokenizer\n",
    "hi_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "hi_vocab_size = len(hi_tokenizer.word_index) + 1\n",
    "hi_length = max_length(dataset[:, 0])\n",
    "print('Hinglish Vocabulary Size: %d' % hi_vocab_size)\n",
    "print('Hinglish Max Length: %d' % (hi_length))\n",
    "# prepare german tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 1])\n",
    "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
    "print('English Max Length: %d' % (eng_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8802212d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode and pad sequences\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "\t# integer encode sequences\n",
    "\tX = tokenizer.texts_to_sequences(lines)\n",
    "\t# pad sequences with 0 values\n",
    "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
    "\treturn X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af4f7b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode target sequence\n",
    "def encode_output(sequences, vocab_size):\n",
    "\tylist = list()\n",
    "\tfor sequence in sequences:\n",
    "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "\t\tylist.append(encoded)\n",
    "\ty = array(ylist)\n",
    "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "\treturn y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2c5d0ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training data\n",
    "trainX = encode_sequences(eng_tokenizer, eng_length, train[:, 1])\n",
    "trainY = encode_sequences(hi_tokenizer, hi_length, train[:, 0])\n",
    "trainY = encode_output(trainY, hi_vocab_size)\n",
    "# prepare validation data\n",
    "testX = encode_sequences(eng_tokenizer, eng_length, test[:, 1])\n",
    "testY = encode_sequences(hi_tokenizer, hi_length, test[:, 0])\n",
    "testY = encode_output(testY, hi_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e0020449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 13, 256)           462592    \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 256)               525312    \n",
      "                                                                 \n",
      " repeat_vector_2 (RepeatVec  (None, 6, 256)            0         \n",
      " tor)                                                            \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 6, 256)            525312    \n",
      "                                                                 \n",
      " time_distributed_2 (TimeDi  (None, 6, 2100)           539700    \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2052916 (7.83 MB)\n",
      "Trainable params: 2052916 (7.83 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define NMT model\n",
    "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
    "\tmodel.add(LSTM(n_units))\n",
    "\tmodel.add(RepeatVector(tar_timesteps))\n",
    "\tmodel.add(LSTM(n_units, return_sequences=True))\n",
    "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "\treturn model\n",
    "\n",
    "# define model\n",
    "model = define_model(eng_vocab_size, hi_vocab_size, eng_length, hi_length, 256)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "# summarize defined model\n",
    "print(model.summary())\n",
    "#plot_model(model, to_file='model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c0125dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 3.16296, saving model to model.h5\n",
      "113/113 - 42s - loss: 4.0129 - val_loss: 3.1630 - 42s/epoch - 370ms/step\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 3.16296 to 2.81677, saving model to model.h5\n",
      "113/113 - 32s - loss: 2.9020 - val_loss: 2.8168 - 32s/epoch - 279ms/step\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 2.81677 to 2.54945, saving model to model.h5\n",
      "113/113 - 31s - loss: 2.5718 - val_loss: 2.5494 - 31s/epoch - 277ms/step\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 2.54945 to 2.35021, saving model to model.h5\n",
      "113/113 - 30s - loss: 2.3094 - val_loss: 2.3502 - 30s/epoch - 266ms/step\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 2.35021 to 2.20002, saving model to model.h5\n",
      "113/113 - 31s - loss: 2.0961 - val_loss: 2.2000 - 31s/epoch - 277ms/step\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 2.20002 to 2.06280, saving model to model.h5\n",
      "113/113 - 31s - loss: 1.9108 - val_loss: 2.0628 - 31s/epoch - 272ms/step\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 2.06280 to 1.93243, saving model to model.h5\n",
      "113/113 - 32s - loss: 1.7479 - val_loss: 1.9324 - 32s/epoch - 281ms/step\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 1.93243 to 1.81778, saving model to model.h5\n",
      "113/113 - 30s - loss: 1.5953 - val_loss: 1.8178 - 30s/epoch - 268ms/step\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 1.81778 to 1.73379, saving model to model.h5\n",
      "113/113 - 31s - loss: 1.4570 - val_loss: 1.7338 - 31s/epoch - 279ms/step\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 1.73379 to 1.66342, saving model to model.h5\n",
      "113/113 - 30s - loss: 1.3318 - val_loss: 1.6634 - 30s/epoch - 269ms/step\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 1.66342 to 1.58426, saving model to model.h5\n",
      "113/113 - 32s - loss: 1.2178 - val_loss: 1.5843 - 32s/epoch - 281ms/step\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 1.58426 to 1.53056, saving model to model.h5\n",
      "113/113 - 30s - loss: 1.1189 - val_loss: 1.5306 - 30s/epoch - 266ms/step\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 1.53056 to 1.50865, saving model to model.h5\n",
      "113/113 - 32s - loss: 1.0292 - val_loss: 1.5086 - 32s/epoch - 280ms/step\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 1.50865 to 1.45474, saving model to model.h5\n",
      "113/113 - 30s - loss: 0.9471 - val_loss: 1.4547 - 30s/epoch - 268ms/step\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss improved from 1.45474 to 1.42137, saving model to model.h5\n",
      "113/113 - 31s - loss: 0.8726 - val_loss: 1.4214 - 31s/epoch - 278ms/step\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss improved from 1.42137 to 1.40241, saving model to model.h5\n",
      "113/113 - 30s - loss: 0.8047 - val_loss: 1.4024 - 30s/epoch - 268ms/step\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 1.40241 to 1.38454, saving model to model.h5\n",
      "113/113 - 32s - loss: 0.7469 - val_loss: 1.3845 - 32s/epoch - 282ms/step\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss improved from 1.38454 to 1.36605, saving model to model.h5\n",
      "113/113 - 31s - loss: 0.6908 - val_loss: 1.3661 - 31s/epoch - 271ms/step\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss improved from 1.36605 to 1.36515, saving model to model.h5\n",
      "113/113 - 32s - loss: 0.6344 - val_loss: 1.3651 - 32s/epoch - 283ms/step\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss improved from 1.36515 to 1.34902, saving model to model.h5\n",
      "113/113 - 32s - loss: 0.5846 - val_loss: 1.3490 - 32s/epoch - 283ms/step\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss improved from 1.34902 to 1.33405, saving model to model.h5\n",
      "113/113 - 33s - loss: 0.5418 - val_loss: 1.3340 - 33s/epoch - 295ms/step\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss did not improve from 1.33405\n",
      "113/113 - 31s - loss: 0.4990 - val_loss: 1.3490 - 31s/epoch - 270ms/step\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss did not improve from 1.33405\n",
      "113/113 - 32s - loss: 0.4622 - val_loss: 1.3379 - 32s/epoch - 285ms/step\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss improved from 1.33405 to 1.32949, saving model to model.h5\n",
      "113/113 - 30s - loss: 0.4280 - val_loss: 1.3295 - 30s/epoch - 267ms/step\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss improved from 1.32949 to 1.32535, saving model to model.h5\n",
      "113/113 - 32s - loss: 0.3958 - val_loss: 1.3254 - 32s/epoch - 282ms/step\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss improved from 1.32535 to 1.32271, saving model to model.h5\n",
      "113/113 - 33s - loss: 0.3652 - val_loss: 1.3227 - 33s/epoch - 288ms/step\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss did not improve from 1.32271\n",
      "113/113 - 33s - loss: 0.3365 - val_loss: 1.3232 - 33s/epoch - 292ms/step\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss did not improve from 1.32271\n",
      "113/113 - 30s - loss: 0.3142 - val_loss: 1.3283 - 30s/epoch - 263ms/step\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss did not improve from 1.32271\n",
      "113/113 - 32s - loss: 0.2931 - val_loss: 1.3253 - 32s/epoch - 281ms/step\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss did not improve from 1.32271\n",
      "113/113 - 30s - loss: 0.2729 - val_loss: 1.3328 - 30s/epoch - 264ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1d053e6b190>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "filename = 'model.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "model.fit(trainX, trainY, epochs=30, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c760b0",
   "metadata": {},
   "source": [
    "# Evaluate Neural Translation Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "36136535",
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "# load datasets\n",
    "dataset = load_clean_sentences('english-hi-both.pkl')\n",
    "train = load_clean_sentences('english-hi-train.pkl')\n",
    "test = load_clean_sentences('english-hi-test.pkl')\n",
    "# prepare english tokenizer\n",
    "hi_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "hi_vocab_size = len(hi_tokenizer.word_index) + 1\n",
    "hi_length = max_length(dataset[:, 0])\n",
    "# prepare german tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 1])\n",
    "# prepare data\n",
    "trainX = encode_sequences(eng_tokenizer, eng_length, train[:, 1])\n",
    "testX = encode_sequences(eng_tokenizer, eng_length, test[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9543e24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "afa87159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "\tfor word, index in tokenizer.word_index.items():\n",
    "\t\tif index == integer:\n",
    "\t\t\treturn word\n",
    "\treturn None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f7de97f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate target given source sequence\n",
    "def predict_sequence(model, tokenizer, source):\n",
    "\tprediction = model.predict(source, verbose=0)[0]\n",
    "\tintegers = [argmax(vector) for vector in prediction]\n",
    "\ttarget = list()\n",
    "\tfor i in integers:\n",
    "\t\tword = word_for_id(i, tokenizer)\n",
    "\t\tif word is None:\n",
    "\t\t\tbreak\n",
    "\t\ttarget.append(word)\n",
    "\treturn ' '.join(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f536b31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the skill of the model\n",
    "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
    "\tactual, predicted = list(), list()\n",
    "\tfor i, source in enumerate(sources):\n",
    "\t\t# translate encoded source text\n",
    "\t\tsource = source.reshape((1, source.shape[0]))\n",
    "\t\ttranslation = predict_sequence(model, hi_tokenizer, source)\n",
    "\t\traw_target, raw_src = raw_dataset[i]\n",
    "\t\tif i < 10:\n",
    "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
    "\t\tactual.append([raw_target.split()])\n",
    "\t\tpredicted.append(translation.split())\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "# calculate BLEU score\n",
    "#print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "#print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "#print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "#print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59eca7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
    " actual, predicted = list(), list()\n",
    " for i, source in enumerate(sources):\n",
    " # translate encoded source text\n",
    " source = source.reshape((1, source.shape[0]))\n",
    " translation = predict_sequence(model, eng_tokenizer, source)\n",
    " raw_target, raw_src = raw_dataset[i]\n",
    " if i < 10:\n",
    " print('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
    " actual.append([raw_target.split()])\n",
    " predicted.append(translation.split())\n",
    " # calculate BLEU score\n",
    " print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    " print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    " print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    " print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9a75b3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "dataset = load_clean_sentences('english-hi-both.pkl')\n",
    "train = load_clean_sentences('english-hi-train.pkl')\n",
    "test = load_clean_sentences('english-hi-test.pkl')\n",
    "# prepare english tokenizer\n",
    "hi_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "hi_vocab_size = len(hi_tokenizer.word_index) + 1\n",
    "hi_length = max_length(dataset[:, 0])\n",
    "# prepare german tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 1])\n",
    "# prepare data\n",
    "trainX = encode_sequences(eng_tokenizer, eng_length, train[:, 1])\n",
    "testX = encode_sequences(eng_tokenizer, eng_length, test[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "abbd9a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "src=[events in concord], target=[concord mei events], predicted=[movie mei please]\n",
      "src=[repeat song], target=[song ko repeat kare], predicted=[song ko repeat kare]\n",
      "src=[pause timer please], target=[timer pause kare], predicted=[timer pause kare]\n",
      "src=[traffic report], target=[traffic report], predicted=[traffic love]\n",
      "src=[the imitation game], target=[nakalee khel], predicted=[bot me me he]\n",
      "src=[message i can t get off work], target=[i can t get off work], predicted=[kya alarm alarm alarm hai]\n",
      "src=[monsters university], target=[monsters university], predicted=[fall ke]\n",
      "src=[only country], target=[sirf country hai], predicted=[kya abhi acha hai]\n",
      "src=[go away alarm], target=[alarm ko band karo], predicted=[ek alarm alarm lagaye]\n",
      "src=[will it rain today], target=[kya aj barish hogi], predicted=[kya aj baarish hogi]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model = load_model('model.h5')\n",
    "# test on some training sequences\n",
    "print('train')\n",
    "evaluate_model(model, hi_tokenizer, trainX, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "edb8b9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: song ko repeat karee\n",
      "Translation: what the timer\n"
     ]
    }
   ],
   "source": [
    "def translate_sentence(model, tokenizer, sentence):\n",
    "    # Tokenize the source sentence\n",
    "    source_seq = tokenizer.texts_to_sequences([sentence])[0]\n",
    "    source_seq = np.array(source_seq).reshape((1, len(source_seq)))\n",
    "\n",
    "    # Use the predict_sequence function to generate the translation\n",
    "    translation = predict_sequence(model, tokenizer, source_seq)\n",
    "\n",
    "    return translation\n",
    "\n",
    "# Example usage:\n",
    "source_sentence = \"song ko repeat karee\"\n",
    "translation = translate_sentence(model, eng_tokenizer, source_sentence)\n",
    "print(f'Source: {source_sentence}')\n",
    "print(f'Translation: {translation}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "52d8ef1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'is', 'timer', 'it', 'alarm', 'play', 'today', 'song', 'what', 'will', 'weather', 'music', 'stop', 'off', 'how', 'turn', 's', 'to', 'pause', 'next', 'when', 'traffic', 'on', 'be', 'skip', 'a', 'i', 'this', 'set', 'in', 'rain', 'please', 'alarms', 'like', 'my', 'tomorrow', 'start', 'time', 'cancel', 'events', 'track', 'hello', 'you', 'cold', 'snooze', 'hot', 'there', 'outside', 'up', 'are', 'me', 'show', 'new', 'now', 'hi', 'snow', 'some', 'yes', 'repeat', 'change', 'that', 'sunny', 'text', 'an', 'send', 'at', 'songs', 'for', 'pm', 'shut', 'chats', 'delete', 'whats', 'reset', 'add', 'out', 'message', 'forecast', 'go', 'temperature', 'get', 'high', 'rock', 'all', 'end', 'going', 'much', 'no', 'concerts', 'from', 'have', 'make', 'current', 'good', 'oh', 'not', 'resume', 'yeah', 'right', 'restart', 'minutes', 'windy', 'am', 'too', 'radio', 'top', 'pop', 'more', 'of', 'do', 'parties', 'left', 'hey', 'does', 'reply', 'sunrise', 'jazz', 'movie', 'rap', 'was', 'playlist', 'quiet', 'day', 'tonight', 'by', 'low', 'shuffle', 'tell', 'humid', 'did', 'find', 'create', 'ok', 'sun', 'where', 'stopwatch', 'timers', 'thats', 'reminder', 'previous', 'clear', 'lol', 'give', 'nice', 'humidity', 'who', 'long', 'bye', 'its', 'cool', 'remove', 'r', 'temp', 'cloudy', 'any', 'country', 'down', 'warm', 'one', 'switch', 'replay', 'view', 'happening', 'iheartradio', 'agree', 'haha', 'great', 'why', 'so', 'begin', 'hits', 'channel', 'here', 'and', 'really', 'weekend', 'seen', 'over', 'read', 'mom', 'take', 'about', 'christmas', 'bring', 'put', 'sound', 'roads', 'back', 'open', 'update', 'last', 't', 'only', 'index', 'u', 'listen', 'okay', 'home', 'again', 'done', 'love', 'b', 'say', 'close', 'let', 'im', 'report', 'station', 'reminders', 'something', 'don', 'sunset', 'know', 'think', 'many', 'another', 'work', 'festivals', 'need', 'wake', 'near', 'yea', 'backed', 'john', 'remaining', 'raining', 'morning', 'hip', 'hop', 'hour', 'chat', 'look', 'uv', 'texts', 'see', 'eta', 'sure', 'slow', 'messages', 'hate', 'wine', 'playing', 'well', 'live', 'mute', 'or', 'minute', 'york', 'free', 'ice', 'sms', 'sad', 'come', 'he', 'concert', 'thanks', 'wow', 'quit', 'route', 'also', 'video', 'seconds', 'local', 'doing', 'week', 'respond', 'la', 'drake', 'icy', 'far', 'fun', 'holiday', 'hit', 'display', 'write', 'halt', 'm', 'pink', 'erase', 'dad', 'funny', 'jam', 'definitely', 'enough', 'times', 'countdown', 'hold', 'watch', 'her', 'heat', 'silence', 'has', 'ringing', 'kids', 'check', 'accidents', 'loop', 'nyc', 'requests', 'we', 'timing', 'eminem', 'lap', 'dog', 'movies', 'point', 'shows', 'beyonce', 'year', 'south', 'yesterday', 'chance', 'true', 'may', 'sounds', 'would', 'chilly', 'with', 'away', 'noise', 'wind', 'list', 'later', 'first', 'lane', 'thank', 'metal', 'dont', 'dew', 'latest', 'pretty', 'speed', 'limit', 'mix', 'pandora', 'interesting', 'delays', 'run', 'delay', 'name', 'night', 'been', 'miami', 'sleep', 'happy', 'same', 'longer', 'bob', 'animated', 'nope', 'june', 'party', 'wet', 'min', 'hows', 'pull', 'favorites', 'havent', 'sorry', 'thumbs', 'playback', 'two', 'awesome', 'can', 'freeway', 'browse', 'skating', 'just', 'mark', 'todays', 'degrees', 'things', 'ring', 'weekly', 'disney', 'keep', 'relaxing', 'kesha', 'number', 'dusk', 'friday', 'comedy', 'exit', 'classic', 'move', 'vegas', 'detour', 'which', 'sunday', 'acdc', 'very', 'until', 'road', 'still', 'cut', 'busy', 'blues', 'old', 'roll', 'tasting', 'stormy', 'days', 'hahaha', 'face', 'man', 'hours', 'detours', 'tone', 'shush', 'sense', 'radar', 'n', 'fall', 'favorite', 'similar', 'sam', 'correct', 'silent', 'highway', 'photo', 'hahah', 'tomorrows', 'gym', 'remember', 'monday', 'umbrella', 'despacito', 'she', 'family', 'fine', 'town', 'spotify', 'kid', 'alone', 'clock', 'snowy', 'ahead', 'best', 'parades', 'upbeat', 'picture', 'ah', 'him', 'if', 'tampa', 'info', 'bluegrass', 'quite', 'eh', 'oldies', 'course', 'genre', 'denver', 'rush', 'closed', 'ask', 'las', 'commute', 'maroon', 'answer', 'journey', 'beach', 'kpop', 'they', 'supposed', 'dawn', 'k', 'smooth', 'event', 'lets', 'tastings', 'bad', 'd', 'exactly', 'edit', 'cease', 'maybe', 'anita', 'baker', 'fair', 'way', 'situation', 'mins', 'boston', 'director', 'break', 'street', 'directions', 'houston', 'park', 'cant', 'soothing', 'sweater', 'trap', 'yet', 'type', 'midnight', 'huh', 'santa', 'seasonal', 'ya', 'saw', 'london', 'cher', 'either', 'edm', 'art', 'bruno', 'mars', 'image', 'orlando', 'pearl', 'clouds', 'ten', 'elvis', 'dinner', 'indeed', 'shot', 'uptown', 'anna', 'recent', 'parade', 'awake', 'partner', 'sundown', 'must', 'already', 'album', 'china', 'setting', 'mesa', 'big', 'jacket', 'yours', 'three', 'costume', 'record', 'made', 'rating', 'school', 'nearby', 'closures', 'nightlife', 'painting', 'cookie', 'totally', 'moms', 'fix', 'paused', 'tours', 'download', 'bot', 'film', 'cast', 'storm', 'tho', 'folk', 'upcoming', 'reroute', 'bands', 'never', 'ignore', 'dallas', 'action', 'as', 'metallica', 'currently', 'outlook', 'fan', 'response', 'imdb', 'want', 'plan', 'dc', 'soundtrack', 'them', 'tom', 'shoot', 'words', 'rihanna', 'dry', 'makes', 'got', 'michael', 'tuesday', 'every', 'maxwell', 'beatles', 'real', 'reviews', 'stopped', 'your', 'story', 'yup', 'lullaby', 'rainy', 'frost', 'muggy', 'rise', 'chill', 'children', 'star', 'shining', 'pantera', 'rest', 'save', 'tunes', 'doze', 'anything', 'but', 'usher', 'random', 'abort', 'human', 'bart', 'gavyn', 'toothless', 'hail', 'hugh', 'jackman', 'batman', 'superman', 'boyz', 'hov', 'needed', 'our', 'ugly', 'hmm', 'grandma', 'fell', 'remains', 'alert', 'va', 'reading', 'walking', 'years', 'acting', 'yep', 'team', 'joss', 'whedon', 'construction', 'romantic', 'hunt', 'tempe', 'lows', 'sports', 'outdoor', 'rename', 'drama', 'light', 'helen', 'florida', 'idle', 'indie', 'beeping', 'area', 'then', 'future', 'thoughts', 'month', 'toronto', 'toll', 'texted', 'dewpoint', 'funk', 'newest', 'above', 'return', 'nickelback', 'moscow', 'portugal', 'tide', 'san', 'line', 'updates', 'kill', 'continue', 'study', 'dan', 'twice', 'hwy', 'east', 'surf', 'status', 'peter', 'swan', 'iowa', 'reports', 'hourly', 'volume', 'police', 'count', 'lady', 'gaga', 'zip', 'jack', 'friendly', 'yellow', 'kidz', 'bop', 'both', 'liked', 'kind', 'else', 'happens', 'prince', 'intresting', 'lyle', 'downstairs', 'leo', 'alternative', 'routes', 'person', 'stereo', 'absolutely', 'making', 'modify', 'mode', 'probably', 'bus', 'nsw', 'fog', 'travel', 'since', 'rained', 'dancing', 'flood', 'ontario', 'toy', 'causing', 'remind', 'contests', 'steel', 'should', 'appointment', 'j', 'cole', 'retired', 'goodnight', 'tracks', 'berlin', 'news', 'march', 'classical', 'schools', 'car', 'crash', 'watched', 'accident', 'adjust', 'instrumental', 'bobby', 'instead', 'rotten', 'tomatoes', 'asian', 'voice', 'moana', 'tupac', 'gaslight', 'rainfall', 'generate', 'though', 'luke', 'power', 'rated', 'fairs', 'happen', 'hottest', 'than', 'blue', 'stars', 'search', 'paint', 'played', 'motown', 'atlanta', 'ohk', 'celsius', 'hide', 'care', 'spa', 'queen', 'bill', 'idea', 'alexa', 'locally', 'these', 'caine', 'katie', 'madonna', 'australia', 'coming', 'heart', 'place', 'actually', 'hanks', 'examples', 'west', 'festival', 'located', 'miles', 'were', 'gospel', 'hard', 'snowing', 'heavy', 'christian', 'sting', 'ive', 'had', 'understand', 'e', 'leave', 'passed', 'mine', 'character', 'jrock', 'talking', 'workshops', 'thursday', 'abc', 'dnce', 'st', 'portland', 'merge', 'most', 'streep', 'adele', 'jump', 'fast', 'forward', 'gone', 'texas', 'hurricane', 'jams', 'seattle', 'bruce', 'boy', 'messenger', 'july', 'mum', 'topic', 'before', 'rodeo', 'actor', 'reggae', 'cheap', 'fire', 'sending', 'tool', 'nicks', 'ending', 'hes', 'nas', 'suppose', 'little', 'thumb', 'house', 'iron', 'concord', 'imitation', 'game', 'monsters', 'university', 'anyways', 'sum', 'sue', 'reopen', 'lyrics', 'ji', 'sia', 'yesi', 'calvin', 'incubus', 'whatever', 'always', 'joanne', 'milan', 'paris', 'wpoc', 'toddler', 'whole', 'kpaw', 'nkotb', 'steven', 'section', 'incometax', 'act', 'doors', 'tupperware', 'race', 'language', 'math', 'pollen', 'level', 'jane', 'predicted', 'biloxi', 'vs', 'melbourne', 'hawaii', 'unfortunate', 'zoo', 'vap', 'combine', 'black', 'bbd', 'notebook', 'expire', 'wham', 'pumpkin', 'whaaaaat', 'required', 'soundscapes', 'harscabble', 'issues', 'auctioneer', 'breeze', 'storms', 'banana', 'phone', 'document', 'capleton', 'shh', 'von', 'braun', 'casting', 'clubs', 'nothing', 'guess', 'blocked', 'nerdiest', 'looking', 'red', 'box', 'each', 'their', 'own', 'elmo', 'dance', 'siri', 'social', 'network', 'aa', 'meetings', 'mdot', 'rochester', 'ooh', 'youre', 'heyy', 'cure', 'seem', 'roanoke', 'nellis', 'afb', 'tuberculosis', 'q', 'acacia', 'strain', 'kmiy', 'mike', 'cams', 'bland', 'chappie', 'expositions', 'ballads', 'floyd', 'ohok', 'adults', 'sewee', 'saloons', 'ricky', 'nelson', 'mirren', 'happenings', 'midday', 'wrdg', 'halls', 'easy', 'listening', 'taste', 'starting', 'zero', 'drive', 'ratings', 'massage', 'training', 'retrieve', 'ozzy', 'wins', 'mcu', 'steffani', 'howells', 'final', 'alyse', 'ofcourse', 'checkpoints', 'coproduced', 'monica', 'lovely', 'december', 'through', 'virginia', 'tesla', 'arthur', 'princess', 'roast', 'dude', 'downloads', 'carey', 'mulligandaisy', 'waiting', 'different', 'swim', 'food', 'korean', 'rescue', 'bout', 'diego', 'german', 'fest', 'opera', 'incredibles', 'understood', 'mass', 'kansas', 'nivce', 'sweet', 'styx', 'childrens', 'hellooo', 'opening', 'alright', 'plays', 'breezy', 'lead', 'actors', 'jay', 'z', 'snowies', 'ludacris', 'places', 'evening', 'shutdown', 'chainz', 'thomas', 'h', 'especially', 'winds', 'father', 'freezing', 'conditions', 'lectures', 'kiyoharu', 'adam', 'contest', 'buffalo', 'everyday', 'bless', 'frozen', 'broadway', 'fighting', 'robots', 'tathra', 'nah', 'comic', 'conventions', 'post', 'elevator', 'bahamas', 'mold', 'jackie', 'playlists', 'interested', 'neither', 'usually', 'together', 'karen', 'boat', 'violence', 'wondering', 'dvds', 'viera', 'marie', 'expect', 'theme', 'bessy', 'youd', 'firstworldproblems', 'lennon', 'freakin', 'downs', 'miguel', 'kiss', 'mozart', 'tha', 'code', 'likewise', 'flooding', 'mess', 'thre', 'stuff', 'alternate', 'bet', 'sometimes', 'amend', 'england', 'ryan', 'gosling', 'romance', 'overcast', 'saying', 'entry', 'ben', 'grunge', 'dirty', 'mole', 'amazing', 'cook', 'sunshine', 'jogging', 'created', 'detailed', 'reject', 'holdup', 'screenings', 'bridge', 'fastest', 'turnpine', 'ladygaga', 'marvel', 'raincoat', 'unlike', 'fen', 'bachata', 'sex', 'lacey', 'cracked', 'unfavorite', 'split', 'tee', 'sleeve', 'montreal', 'peace', 'protests', 'fred', 'forget', 'jason', 'npr', 'wrdx', 'cardi', 'rick', 'james', 'angry', 'ready', 'tornado', 'seven', 'dentist', 'bammel', 'full', 'president', 'yikes', 'wiil', 'marathon', 'games', 'spokane', 'stoplights', 'kinda', 'upvote', 'katy', 'perry', 'anyone', 'musicals', 'women', 'extend', 'iloki', 'fm', 'ihi', 'suspended', 'infomation', 'whaddya', 'cooking', 'classes', 'aerial', 'sry', 'latin', 'someone', 'front', 'annas', 'wwii', 'dayum', 'page', 'sprint', 'thailand', 'drinking', 'slower', 'ted', 'talks', 'dj', 'sets', 'india', 'arie', 'symphony', 'jayz', 'youth', 'scarey', 'perfect', 'punk', 'turkey', 'alter', 'erie', 'example', 'case', 'rodeos', 'tap', 'cheerful', 'maximum', 'animatedsp', 'seems', 'jeezy', 'didn', 'novel', 'canyon', 'acoustic', 'buffet', 'villain', 'columbus', 'daisy', 'worries', 'bopeep', 'marley', 'lower', 'scary', 'because', 'bryan', 'gazing', 'newport', 'phoenix', 'negative', 'opposite', 'tony', 'hamilton', 'showings', 'ms', 'perkins', 'difficult', 'trout', 'boxing', 'pack', 'befriends', 'climax', 'gonna', 'calendar', 'lil', 'wayne', 'discontinue', 'modern', 'other', 'screamo', 'pilots', 'sky', 'umdb', 'artist', 'afternoon', 'kyle', 'mad', 'shared', 'link', 'nite', 'wisconsin', 'getting', 'leon', 'ahaahah', 'boss', 'y', 'humble', 'crazy', 'freeze', 'crowds', 'chainsmokers', 'galleria', 'wrong', 'salsa', 'seabrook', 'libby', 'european', 'caleb', 'happnin', 'vibrate', 'january', 'iran', 'maryl', 'stree', 'spring', 'killer', 'dvd', 'timberlake', 'christopher', 'w', 'alken', 'wctq', 'mariachi', 'tones', 'hola', 'ando', 'leyendo', 'ktcl', 'try', 'die', 'raffi', 'emma', 'stone', 'techno', 'noreaster', 'fight', 'tokyo', 'airport', 'chorus', 'active', 'monsoon', 'could', 'creed', 'turnpike', 'helllo', 'chances', 'lmao', 'haim', 'nonot', 'storyline', 'engage', 'redirect', 'begins', 'oooh', 'took', 'tea', 'twitter', 'dm', 'sp', 'orts', 'handclap', 'february', 'isnt', 'truth', 'uh', 'suspend', 'trip', 'cheery', 'initiate', 'monster', 'pits', 'loki', 'peak', 'siberia', 'pierce', 'county', 'lucas', 'phish', 'yesterdays', 'mesages', 'macon', 'shhhhhhh', 'olaf', 'sarah', 'below', 'smh', 'waco', 'tx', 'isn', 'working', 'saturday', 'wasn', 'wing', 'eating', 'macys', 'shape', 'water', 'sarasota', 'psy', 'gangnam', 'style', 'key', 'bff', 'durham', 'buzzing', 'panting', 'coldest', 'chicago', 'shall', 'abide', 'men', 'setup', 'arise', 'press', 'eclipes', 'after', 'gloomy', 'driver', 'blackburn', 'imax', 'mm', 'sent', 'dub', 'step', 'produce', 'pay', 'club', 'raise', 'marathons', 'khue', 'mercy', 'replying', 'vancouver', 'shure', 'workout', 'smile', 'signal', 'skiing', 'goodafternoon', 'hanna', 'shane', 'fury', 'tobymac', 'gallery', 'beaches', 'happened', 'four', 'seasons', 'oktoberfest', 'blizzard', 'max', 'excellent', 'talk', 'aborting', 'lame', 'opinions', 'business', 'cake', 'anthems', 'activity', 'mma', 'chronos', 'drop', 'mall', 'didnt', 'poetry', 'almost', 'pete', 'alerts', 'seeing', 'load', 'andover', 'louis', 'wlqb', 'bar', 'crawl', 'allergy', 'released', 'lax', 'akota', 'goyo', 'bingo', 'war', 'conflict', 'hahahaha', 'graphics', 'bodak', 'sammy', 'latino', 'side', 'original', 'alaska', 'dmx', 'funhouse', 'super', 'viggo', 'main', 'part', 'deep', 'pa', 'lisa', 'cent', 'cinemascore', 'ukraine', 'vivaldi', 'california', 'us', 'island', 'gotcha', 'skrillex', 'mean', 'dragon', 'con', 'while', 'ago', 'timeout', 'yoga', 'wick', 'said', 'soul', 'walt', 'dayton', 'stats', 'definitly', 'fiery', 'foods', 'hiphop', 'word', 'lucky', 'cgi', 'frank', 'abagnale', 'enjoy', 'quick', 'girl', 'slick', 'slinky', 'rex', 'struggle', 'downtown', 'into', 'those', 'papers', 'ozone', 'winklevoss', 'twins', 'canada', 'ooohhh', 'fuck', 'em', 'ufc', 'quieten', 'interstate', 'brunch', 'bathtub', 'along', 'armor', 'swing', 'songlist', 'ohio', 'total', 'wuss', 'johnny', 'cash', 'train', 'excursions', 'billion', 'relevant', 'springsteen', 'rowlett', 'dislike', 'likely', 'nature', 'pampering', 'statement', 'crew', 'spots', 'audio', 'pta', 'condition', 'bon', 'jovi', 'sally', 'nopes', 'halloween', 'metacritic', 'score', 'chandler', 'sza', 'grandpa', 'whale', 'rainbow', 'formed', 'righy', 'tonigh', 'wouldnt', 'tonights', 'malls', 'meryl', 'leonardo', 'dicaprio', 'lose', 'yourself', 'reps', 'liverpool', 'craft', 'mandisa', 'hjy', 'chime', 'ding', 'bedtime', 'oher', 'odd', 'returns', 'philly', 'scrapbooking', 'rebecca', 'library', 'drivename', 'weight', 'lifting', 'fantasy', 'eclipse', 'hollywood', 'boise', 'bridges', 'ideas', 'mics', 'toledo', 'pour', 'glitch', 'mob', 'hang', 'busta', 'rhymes', 'help', 'krpt', 'steely', 'storming', 'liking', 'poor', 'tim', 'meadows', 'ans', 'twentyeight', 'solid', 'soca', 'glay', 'fwfwfwfw', 'eagan', 'mn', 'popular', 'coat', 'material', 'hospital', 'alsrm', 'shes', 'legend', 'expected', 'sequel', 'downer', 'korn', 'addison', 'loris', 'bone', 'thugs', 'senior', 'celebrations', 'kzrx', 'appears', 'industrial', 'affleckbale', 'tucson', 'jc', 'penney', 'few', 'maine', 'extra', 'wiccan', 'texting', 'five', 'scenery', 'spanish', 'msi', 'ichabod', 'bored', 'cousin', 'confused', 'second', 'plot', 'crush', 'shine', 'shower', 'recipe', 'hear', 'duets', 'grey', 'prices', 'closure', 'matchbox', 'almighty', 'avenue', 'shawn', 'levy', 'being', 'ray', 'names', 'yeap', 'therefore', 'rare', 'experienced', 'cocktail', 'trueeeeee', 'nirvana', 'goodness', 'outkast', 'mild', 'homw', 'olivia', 'north', 'stay', 'fox', 'humor', 'adel', 'standup', 'chatting', 'premieres', 'cute', 'avengers', 'laugh', 'allentown', 'mandissa', 'lightning', 'onto', 'mia', 'aalram', 'important', 'daily', 'history', 'react', 'running', 'buzz', 'bed', 'hair', 'tailgating', 'musical', 'bahama', 'blink', 'series', 'ac', 'ghosts', 'came', 'clarks', 'javier', 'sadkljhn', 'base', 'bro', 'soundgarden', 'satirical', 'crowded', 'cape', 'cod', 'stevie', 'paw', 'patrol', 'hmmm', 'saloon', 'selfie', 'heyo', 'pg', 'plastics', 'earn', 'budget', 'mil', 'races', 'reduce', 'particularly', 'bars', 'jose', 'winter', 'undo', 'safe', 'coolio', 'gun', 'wise', 'asap', 'wavw', 'babies', 'bach', 'cd', 'deepak', 'gimmie', 'charlotte', 'cathy', 'alicia', 'keys', 'beep', 'beginning', 'buy', 'rolling', 'stones', 'terrible', 'college', 'hoii', 'population', 'russia', 'gif', 'preset', 'writing', 'caroling', 'kristoff', 'rhianna', 'cameroon', 'dear', 'birthday', 'jinx', 'alan', 'turing', 'c', 'fairly', 'advisory', 'chocolate', 'whos', 'hats', 'moving']\n"
     ]
    }
   ],
   "source": [
    "print(list(eng_tokenizer.word_index.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7ddba497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "src=[stop the alarm], target=[alarm ko stop kare], predicted=[alarm ko stop kare]\n",
      "src=[hello], target=[hello], predicted=[hello]\n",
      "src=[show alarms i have], target=[mere alarms dikhao], predicted=[mere ke liye]\n",
      "src=[hi], target=[hi], predicted=[hi]\n",
      "src=[alarm in], target=[me alarm], predicted=[ghante ka]\n",
      "src=[i am not there], target=[mai vaha nahi hu], predicted=[mai yaad nahi hu]\n",
      "src=[okay why did you leave], target=[ok aap kyu gaye the], predicted=[free reviews bhi]\n",
      "src=[hello], target=[hello], predicted=[hello]\n",
      "src=[skip to the next song], target=[agle song par jaayen], predicted=[agle song par jaayen]\n",
      "src=[repeat track], target=[track ko repeat kare], predicted=[track ko repeat kare]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test on some test sequences\n",
    "print('test')\n",
    "evaluate_model(model, hi_tokenizer, testX, test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
